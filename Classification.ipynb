{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u><b> League of Legends winner by 10 minutes</b></u>\n",
    "\n",
    "In this notebook we try to make a model that can predict the winner in a hugh elo (high diamond) game of league of legends using classification.\n",
    "\n",
    "The paramaters that we are using are as follows:\n",
    "\n",
    "* gameId - Unique RIOT ID of the game. Can be used with the Riot Games API\n",
    "* blueWins - The target column. 1 if the blue team has won, 0 otherwise\n",
    "* blueWardsPlaced - Number of warding totems placed by the blue team on the map\n",
    "* blueWardsDestroyed - Number of enemy warding totems the blue team has destroyed\n",
    "* blueFirstBlood - First kill of the game. 1 if the blue team did the first kill, 0 otherwise\n",
    "* blueKills - Number of enemies killed by the blue team\n",
    "* blueDeaths - Number of deaths (blue team)\n",
    "* blueAssists - Number of kill assists (blue team)\n",
    "* blueEliteMonsters - Number of elite monsters killed by the blue team (Dragons and Heralds)\n",
    "* blueDragons - Number of dragons killed by the blue team\n",
    "* blueHeralds - Number of heralds killed by the blue team\n",
    "* blueTowersDestroyed - Number of towers destroyed by the blue team\n",
    "* blueTotalGold - Blue team total gold\n",
    "* blueAvgLevel - Blue team average champion level\n",
    "* blueTotalExperience - Blue team total experience\n",
    "* blueTotalMinionsKilled - Blue team total minions killed (CS)\n",
    "* blueTotalJungleMinionsKilled - Blue team total jungle monsters killed\n",
    "* blueGoldDiff - Blue team gold difference compared to the enemy team\n",
    "* blueExperienceDiff - Blue team experience difference compared to the enemy team\n",
    "* blueCSPerMin - Blue team CS (minions) per minute\n",
    "* blueGoldPerMin - Blue team gold per minute\n",
    "* redWardsPlaced - Number of warding totems placed by the red team on the map\n",
    "* redWardsDestroyed - Number of enemy warding totems the red team has destroyed\n",
    "* redFirstBlood - First kill of the game. 1 if the red team did the first kill, 0 otherwise\n",
    "* redKills - Number of enemies killed by the red team\n",
    "* redDeaths - Number of deaths (red team)\n",
    "* redAssists - Number of kill assists (red team)\n",
    "* redEliteMonsters - Number of elite monsters killed by the red team (Dragons and Heralds)\n",
    "* redDragons - Number of dragons killed by the red team\n",
    "* redHeralds - Number of heralds killed by the red team\n",
    "* redTowersDestroyed - Number of towers destroyed by the red team\n",
    "* redTotalGold - Red team total gold\n",
    "* redAvgLevel - Red team average champion level\n",
    "* redTotalExperience - Red team total experience\n",
    "* redTotalMinionsKilled - Red team total minions killed (CS)\n",
    "* redTotalJungleMinionsKilled - Red team total jungle monsters killed\n",
    "* redGoldDiff - Red team gold difference compared to the enemy team\n",
    "* redExperienceDiff - Red team experience difference compared to the enemy team\n",
    "* redCSPerMin - Red team CS (minions) per minute\n",
    "* redGoldPerMin - Red team gold per minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c67d3cd318d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mExtraTreesClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, plot_confusion_matrix, roc_curve, plot_roc_curve\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u><b> Part 1 </b></u>\n",
    "\n",
    "### Adjusting the data for program\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd = 42\n",
    "np.random.seed(rnd)\n",
    "df = pd.read_csv(r'data/high_diamond_ranked_10min.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.columns:\n",
    "    print(\"* \"+str(i)+\" - \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u><b> Part 2 </b></u>\n",
    "\n",
    "### Cleaning and scaling the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing known useless data \n",
    "We know that the game ID has 0 impact on who wins the game so we drop it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"gameId\"],inplace=True)\n",
    "data = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of the correlations between the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = data.corr()\n",
    "plt.figure(figsize=(16,12))\n",
    "sns.heatmap(matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove correlated features\n",
    "Highly correlated data can slow our model while not offering any precision, especialy in games like League of Legends where once you have more kills you (usualy) have more gold that lets you get even more kills and objectives. Some features like redDeaths and blueKills are perfectly correlated so we don't need one of them at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlations(D,mainCorr=None, limit=0.95):\n",
    "    #dataframe of boolean values of D dataFrame correlations, true iff |value|>=0.95\n",
    "    highCorrColl = abs(D.corr())>=limit\n",
    "    colls = []\n",
    "    remove = []\n",
    "    for coll in highCorrColl.columns:\n",
    "        #we might delete a column we didn't visit\n",
    "        if coll in highCorrColl.columns:\n",
    "            #if any of the values is true and its not the same index as the column (since at (x,x) we always have a value of 1 so it will always be true)\n",
    "            if highCorrColl[coll][highCorrColl[coll].index!=coll].any():\n",
    "                #mainCorr is a series of correlation values of the main feature and the rest of the columns \n",
    "                if mainCorr is not None:\n",
    "                    #get all the elements that have a high correlation with coll (including coll, (coll,coll) is always true) \n",
    "                    correl = highCorrColl[coll]\n",
    "                    correl = correl[correl==True]\n",
    "\n",
    "                    #get the representation of correl in the correlation series mainCorr\n",
    "                    correlVals = mainCorr[correl.index]\n",
    "                    \n",
    "                    #indexs to remove, we want to keep the ones with the highest correlations to mainCorr/\"blueWins\"\n",
    "                    removeIDX = correlVals[abs(correlVals).idxmax() != correlVals.index].index\n",
    "                    highCorrColl = highCorrColl.drop(columns = removeIDX).drop(index = removeIDX)\n",
    "                    remove.extend(removeIDX)\n",
    "                else:\n",
    "                    #if mainCorr was not passed, drop coll\n",
    "                    highCorrColl = highCorrColl.drop(columns = coll).drop(index = coll)\n",
    "                    remove.append(coll)\n",
    "    return D.drop(columns=remove)\n",
    "\n",
    "#custom class for pipeline\n",
    "class FilterCorrelations(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,mainCorr=None,limit=0.95):\n",
    "        self.mainCorr=mainCorr\n",
    "        self.limit = limit\n",
    "    def fit(self, Data):\n",
    "        return self\n",
    "    def transform(self,Data):\n",
    "        return correlations(Data,mainCorr=self.mainCorr, limit=self.limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale data\n",
    "We will use sklearn's standard scaler since our data has many outliers since sometimes player \"throw\" a game resulting in an unexpected win or loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom class for pipeline\n",
    "class Scale(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, Data):\n",
    "        return self\n",
    "    def transform(self,Data):\n",
    "        tempDF = Data.copy()\n",
    "        Data = StandardScaler().fit_transform(Data)\n",
    "        return pd.DataFrame(Data,columns=tempDF.columns,index=tempDF.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find labels wihout correlation with winning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(abs(matrix[\"blueWins\"])<=0).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u><b> Part 3 </b></u>\n",
    "\n",
    "### Splitting data to train/test and making models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = df[\"blueWins\"]\n",
    "data = df.drop(columns=[\"blueWins\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainD, testD, trainE, testE = train_test_split(data, exp,random_state=rnd)\n",
    "pipe = Pipeline([\n",
    "    (\"handle_correlations\",FilterCorrelations(mainCorr=df.corr()[\"blueWins\"],limit=0.95)),\n",
    "    ('scaler',Scale())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainD = pipe.fit_transform(trainD)\n",
    "testD = pipe.transform(testD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding lables with high importance\n",
    "Since we have so many labels, we could try and remove some so the program could run faster without loss of accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = ExtraTreesClassifier(n_estimators=250,random_state=rnd)\n",
    "forest.fit(trainD,trainE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importanceDF = pd.DataFrame({\"label\":trainD.columns, \"importances\":forest.feature_importances_})\n",
    "importanceDF.sort_values(by=\"importances\",ascending=False,inplace=True)\n",
    "importanceDF.plot(x=\"label\",y=\"importances\",kind=\"barh\",figsize=(12,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainD.drop(columns=trainD.columns.difference(importanceDF.head(8)[\"label\"]),inplace=True)\n",
    "testD.drop(columns=testD.columns.difference(importanceDF.head(8)[\"label\"]),inplace=True)\n",
    "importanceDF.head(8)[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainD.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K=5\n",
    "This will be our baseline model, we will try to get better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn5_clf = KNeighborsClassifier(n_neighbors=5)\n",
    "knn5_clf.fit(trainD,trainE)\n",
    "knn5_pred = knn5_clf.predict(testD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn5_accuracy = accuracy_score(testE,knn5_pred)\n",
    "print(\"Accuracy:\",knn5_accuracy)\n",
    "print(classification_report(testE,knn5_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paramater tuning the for KNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf = KNeighborsClassifier(n_neighbors=5)\n",
    "#Parameters we want to use in order to find the best KNN model\n",
    "param_grid=[\n",
    "    {'n_neighbors':np.arange(1,31),'weights':[\"uniform\",\"distance\"]}\n",
    "]\n",
    "#change n_jobs for faster/slower performance\n",
    "\n",
    "gridSearch = RandomizedSearchCV(knn_clf, param_grid, n_iter=30, scoring='accuracy', return_train_score=True,verbose=4,n_jobs=-1,random_state=rnd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gridSearch.fit(trainD,trainE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniforms = []\n",
    "distances = []\n",
    "for i in range(len(gridSearch.cv_results_['params'])):\n",
    "    res = gridSearch.cv_results_['params'][i]\n",
    "    #make a dictionary with {n_neighbors,weights,score} and append it in the right list\n",
    "    res['score'] = gridSearch.cv_results_[\"mean_test_score\"][i]\n",
    "    if res[\"weights\"]==\"uniform\": \n",
    "        uniforms.append(res)\n",
    "    else:\n",
    "        distances.append(res)\n",
    "models = np.asarray([uniforms,distances])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the performance of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,12))\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "df1 = pd.DataFrame(models[0]).sort_values(by=\"n_neighbors\")\n",
    "df2 = pd.DataFrame(models[1]).sort_values(by=\"n_neighbors\")\n",
    "ax.plot(df1[\"n_neighbors\"],df1[\"score\"],\"ro-\")\n",
    "ax.plot(df2[\"n_neighbors\"],df2[\"score\"],\"bo-\")\n",
    "#place a green dot at the best classifier\n",
    "ax.plot(gridSearch.best_estimator_.n_neighbors,gridSearch.best_score_,\"go\")\n",
    "ax.set_ylabel(\"mean test score\")\n",
    "ax.set_xlabel(\"n_neighbors\")\n",
    "ax.legend(labels=[\"uniform\",\"distance\",\"best classifier score\"])\n",
    "ax.set_xticks([i for i in range(5*7+1)],minor=True)\n",
    "ax.set_xticks([5*i for i in range(8)],minor=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting KNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf = gridSearch.best_estimator_\n",
    "knn_clf.fit(trainD,trainE)\n",
    "knn_pred = knn_clf.predict(testD)\n",
    "knn_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_accuracy = accuracy_score(testE,knn_pred)\n",
    "print(\"Accuracy:\",knn_accuracy)\n",
    "print(classification_report(testE,knn_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paramater tuning for the SVC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_clf = SVC(random_state=rnd)\n",
    "#Parameters we want to use in order to find the best KNN model\n",
    "param_grid=[\n",
    "    {'C':np.linspace(0.9,1.1,30),\"gamma\":[\"auto\"]},\n",
    "    {'C':np.linspace(0.9,1.1,30),\"gamma\":[\"scale\"]}\n",
    "]\n",
    "#change n_jobs for faster/slower performance\n",
    "gridSearch = RandomizedSearchCV(svc_clf, param_grid, n_iter=20, scoring='accuracy', return_train_score=True,verbose=4,n_jobs=-1,random_state=rnd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridSearch.fit(trainD,trainE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridSearch.cv_results_['params'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = []\n",
    "auto = []\n",
    "for i in range(len(gridSearch.cv_results_['params'])):\n",
    "    res = gridSearch.cv_results_['params'][i]\n",
    "    #make a dictionary with {n_neighbors,weights,score} and append it in the right list\n",
    "    res['score'] = gridSearch.cv_results_[\"mean_test_score\"][i]\n",
    "    if res[\"gamma\"]==\"scale\":\n",
    "        scale.append(res)\n",
    "    else:\n",
    "        auto.append(res)\n",
    "models = np.asarray([scale,auto])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the performance of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,12))\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "df1 = pd.DataFrame(models[0]).sort_values(by=\"C\")\n",
    "df2 = pd.DataFrame(models[1]).sort_values(by=\"C\")\n",
    "ax.plot(df1[\"C\"],df1[\"score\"],\"ro-\")\n",
    "ax.plot(df2[\"C\"],df2[\"score\"],\"bo-\")\n",
    "#place a green dot at the best classifier\n",
    "ax.plot(gridSearch.best_estimator_.C,gridSearch.best_score_,\"go\")\n",
    "ax.set_ylabel(\"mean test score\")\n",
    "ax.set_xlabel(\"C\")\n",
    "ax.legend(labels=[\"gamma=scale\",\"gamma=auto\",\"best classifier score\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting SVC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_clf = gridSearch.best_estimator_\n",
    "svc_clf.fit(trainD,trainE)\n",
    "svc_pred = svc_clf.predict(testD)\n",
    "svc_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_accuracy = accuracy_score(testE,svc_pred)\n",
    "print(\"Accuracy:\",svc_accuracy)\n",
    "print(classification_report(testE,svc_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u><b> Part 4 </b></u>\n",
    "\n",
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(ncols=3,figsize=(20,8),sharex=True)\n",
    "cms = [None,None,None]\n",
    "cms[0] = plot_confusion_matrix(knn5_clf,testD,testE,ax=axes[0],display_labels=[\"Red\",\"Blue\"],values_format=\"d\")\n",
    "cms[1] = plot_confusion_matrix(knn_clf,testD,testE,ax=axes[1],display_labels=[\"Red\",\"Blue\"],values_format=\"d\")\n",
    "cms[2] = plot_confusion_matrix(svc_clf,testD,testE,ax=axes[2],display_labels=[\"Red\",\"Blue\"],values_format=\"d\")\n",
    "vals = [[\"True Neg\",\"False Pos\"],[\"False Neg\",\"True Pos\"]]\n",
    "for cm in cms:\n",
    "    text = [[cm.text_[i][j].get_text() for j in range(len(cm.text_[i]))] for i in range(len(cm.text_))]\n",
    "    for i in range(len(text)):\n",
    "        for t in range(len(text[i])):\n",
    "            txt = vals[i][t] + \"\\n\" + text[i][t] + \"\\n\" + '{0:.3f}'.format(100*(int(text[i][t])/len(testE)))+\"%\"\n",
    "            cm.text_[i][t].set_text(txt)\n",
    "        \n",
    "axes[0].title.set_text(\"KNN\\nn = 5\")\n",
    "axes[1].title.set_text(\"KNN\\nn =\"+str(knn_clf.n_neighbors))\n",
    "axes[2].title.set_text(\"SVC\\nC =\"+str('{0:.2f}'.format(svc_clf.C)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,12))\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "falsePosRate, truePosRate, _ = roc_curve(testE,np.random.choice([0,1],size=len(testE)))\n",
    "plot_roc_curve(knn5_clf,testD,testE,ax=ax,name=\"KNN, n = 5\")\n",
    "plot_roc_curve(knn_clf,testD,testE,ax=ax,name=\"KNN, n =\"+str(knn_clf.n_neighbors))\n",
    "plot_roc_curve(svc_clf,testD,testE,ax=ax,name=\"SVC, C =\"+str('{0:.2f}'.format(svc_clf.C)))\n",
    "ax.plot(falsePosRate, truePosRate, linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy rate of KNN model (n = 5):\",accuracy_score(testE,knn5_pred))\n",
    "print(\"Accuracy rate of KNN model (n = \" + str(knn_clf.n_neighbors) + \"):\",accuracy_score(testE,knn_pred))\n",
    "print(\"Accuracy rate of SVC model (C = \" + str('{0:.2f}'.format(svc_clf.C)) + \"):\",accuracy_score(testE,svc_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u><b> Conclusions </b></u>\n",
    "\n",
    "70%-73% seems like a low accuracy but considering the dataset is based on the first 10 minutes of a game, having anything over 65% is incredibly good!\n",
    "\n",
    "This data shows us that although a large amount of the community think the game is over because of a single mistake at the start of the game, this is not actually the case.\n",
    "\n",
    "In addition, the dataset doesn't list the champions on each team, some chamions are really good at the early part of the game like Renekton and Lee sin, but tend to fall off in the mid-late game while chamions like Veigar and Nasus tend to have a hard time at the early parts of the game, but can solo carry a game if kept unchecked.\n",
    "\n",
    "Having only 70%-73% accuracy score is actually a very good sign! Since it means that players can't know who would win this early, meaning that their performance in the game won't change if they find out they have a 90% chance to lose or win, this can make it very frustrating, even for the winning team because the losing team could choose to surrender at minute 15 and not allow the winning team enjoy their win properly. \n",
    "\n",
    "Since we only have data from the 10 minute mark, we can only get a 70%-73% accuracy score, but if we had data from minute 15 or 20, I would expect an accuracy rate closer to 80%-85%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u><b> Part 5 </b></u>\n",
    "\n",
    "### Improvements\n",
    "\n",
    "Now we will try to imorove our model, lets see if we can reach a better accuracy and at the very least, reduce the amount of features we need to get an accurate result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import time\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns = [\"blueWins\"]), df[\"blueWins\"], random_state=rnd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new = X_train.drop(columns=X_train.columns.difference(importanceDF.head(8)[\"label\"]))\n",
    "X_test_new = X_test.drop(columns=X_train.columns.difference(importanceDF.head(8)[\"label\"]))\n",
    "importanceDF.head(8)[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = Normalizer()\n",
    "X_train_new = scale.fit_transform(X_train_new)\n",
    "X_test_new = scale.transform(X_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = StandardScaler()\n",
    "\n",
    "#X_train_new = scale.fit_transform(X_train_new)\n",
    "#X_test_new = scale.transform(X_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA\n",
    "Let's first start by reducing our model's complexity using PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(random_state=rnd)\n",
    "pca.fit(X_train_new)\n",
    "exp_var_cumul = np.cumsum(pca.explained_variance_ratio_)\n",
    "pca.fit(X_train_new)\n",
    "exp_var_cumul2 = np.cumsum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(1,exp_var_cumul.shape[0] + 1),exp_var_cumul)\n",
    "ax.fill_between(range(1,exp_var_cumul.shape[0] + 1),exp_var_cumul,0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = len(exp_var_cumul[exp_var_cumul<0.95])\n",
    "exp_var_cumul[dim], dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"normalizer\",Normalizer()),\n",
    "    (\"pca\",PCA(n_components=dim,random_state=rnd))\n",
    "],verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new = pipe.fit_transform(X_train_new)\n",
    "X_test_new = pipe.transform(X_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train_new, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test_new, label=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### paramater tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "def param_search(param_grid,train,test,train_labels,test_labels,randomSeed=rnd,nfolds=5,eval_method=\"auc\",num_class=2,num_boost_round=10000,early_stopping_rounds=40,verbose=True):\n",
    "    done = 0\n",
    "    amount = len(param_grid)\n",
    "    params = {\n",
    "        'random_state':randomSeed,\n",
    "        'eval_metric':eval_method,\n",
    "    }\n",
    "    start = time.time()\n",
    "    timer = time.time()\n",
    "    best_eval = -float(\"inf\")\n",
    "    best_eval_train = 0\n",
    "    best_params = {}\n",
    "    best_iter = 0\n",
    "    best_out = ''\n",
    "    for p in param_grid:\n",
    "        out = ''\n",
    "        for key in p:\n",
    "            params[key] = p[key]\n",
    "            out += key + '=' + str(p[key])+', '\n",
    "        out = out[:-2]\n",
    "        \n",
    "        results = xgb.cv(params,train,num_boost_round=num_boost_round,nfold=nfolds,metrics={eval_method},early_stopping_rounds=early_stopping_rounds,seed=randomSeed)\n",
    "        \n",
    "        done +=1 \n",
    "        #get mean score of cv of the training and testing, and get the best iteration\n",
    "        mean_eval_test = results['test-'+eval_method+'-mean'].min()\n",
    "        mean_eval_train = results['test-'+eval_method+'-mean'].min()\n",
    "        iteration = results['test-'+eval_method+'-mean'].argmin()\n",
    "        \n",
    "        #update best parameters\n",
    "        if mean_eval_test > best_eval:\n",
    "            best_eval = mean_eval_test\n",
    "            best_eval_train = mean_eval_train\n",
    "            best_out = ''\n",
    "            best_iter = iteration\n",
    "            for key in p:\n",
    "                best_out += key + '=' + str(p[key])+', '\n",
    "                best_params[key] = p[key]\n",
    "            best_out = best_out[:-2]\n",
    "\n",
    "        #show results of cv\n",
    "        if verbose:\n",
    "            print(\"Parameters: \"+out)\n",
    "            print(\"Mean train \"+eval_method+\": \"+str(mean_eval_train) + \" Mean test \"+eval_method+\": \"+str(mean_eval_test) + \" Iteration: \"+str(iteration))\n",
    "            print(str(done)+\"/\"+str(amount))\n",
    "            \n",
    "            if (time.time()-timer) > 300:\n",
    "                print(str((time.time()-start)/60) +\"m elapsed. Finished \"+str(100*done/amount)+\"%\" )\n",
    "                timer = time.time()\n",
    "            print(\"\")\n",
    "    stop = time.time()\n",
    "    if verbose:\n",
    "        print(\"----------------------------------------\")\n",
    "    print(f\"Tuning time: {(time.time()-start)}s\")\n",
    "    print(\"Best parameters: \"+best_out+\":\")\n",
    "    print(\"Best mean train \"+eval_method+\": \"+str(best_eval_train) + \" Best mean test \"+eval_method+\": \"+str(best_eval) + \" Best iteration: \"+str(iteration))\n",
    "    \n",
    "    #show accuracy score\n",
    "    bst = xgb.train(dict({'random_state':randomSeed,'eval_metric':eval_method,'num_class':num_class},**best_params), train, best_iter+1)\n",
    "    test_preds = bst.predict(test)\n",
    "    train_preds = bst.predict(train)\n",
    "\n",
    "    print(\"Test accuracy:\",accuracy_score(test_labels,test_preds),\"Train accuracy:\",accuracy_score(train_labels,train_preds))\n",
    "    return best_params,best_iter+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_search(param_grid,train,test,train_labels,test_labels,num_class=2,randomSeed=rnd,nfolds=5,eval_method=\"auc\",higher_better=True,num_boost_round=10000,early_stopping_rounds=40,verbose=True):\n",
    "    fig, (ax1,ax2) = plt.subplots(1,2,figsize=(20,10))\n",
    "    done = 0\n",
    "    amount = len(param_grid)\n",
    "    params = {\n",
    "        'random_state':randomSeed,\n",
    "        'eval_metric':eval_method,\n",
    "    }\n",
    "    start = time.time()\n",
    "    timer = time.time()\n",
    "    if higher_better: \n",
    "        best_eval = -float(\"inf\")\n",
    "    else:\n",
    "        best_eval = float(\"inf\")\n",
    "    best_eval_train = 0\n",
    "    best_params = {}\n",
    "    best_iter = 0\n",
    "    best_out = ''\n",
    "    with tqdm(total=len(param_grid)) as pbar:\n",
    "        for p in param_grid:\n",
    "            out = ''\n",
    "            for key in p:\n",
    "                params[key] = p[key]\n",
    "                out += key + '=' + str(p[key])+', '\n",
    "            out = out[:-2]\n",
    "            \n",
    "            results = xgb.cv(params,train,num_boost_round=num_boost_round,nfold=nfolds,metrics={eval_method},early_stopping_rounds=early_stopping_rounds,seed=randomSeed)\n",
    "            \n",
    "            ax1.plot(results.index,results['test-'+eval_method+'-mean'],label=out[:25])\n",
    "            ax2.plot(results.index,results['train-'+eval_method+'-mean'],label=out[:25])\n",
    "            ax1.set_title(\"test\")\n",
    "            ax2.set_title(\"train\")\n",
    "            ax1.legend()\n",
    "            ax2.legend()\n",
    "            \n",
    "            done +=1 \n",
    "            #get mean score of cv of the training and testing, and get the best iteration\n",
    "            if higher_better==False:\n",
    "                mean_eval_test = results['test-'+eval_method+'-mean'].min()\n",
    "                mean_eval_train = results['test-'+eval_method+'-mean'].min()\n",
    "                iteration = results['test-'+eval_method+'-mean'].argmin()\n",
    "            else: \n",
    "                mean_eval_test = results['test-'+eval_method+'-mean'].max()\n",
    "                mean_eval_train = results['test-'+eval_method+'-mean'].max()\n",
    "                iteration = results['test-'+eval_method+'-mean'].argmax()\n",
    "            \n",
    "            #update best parameters\n",
    "            if (mean_eval_test < best_eval and higher_better==False) or (mean_eval_test > best_eval and higher_better):\n",
    "                best_eval = mean_eval_test\n",
    "                best_eval_train = mean_eval_train\n",
    "                best_out = ''\n",
    "                best_iter = iteration\n",
    "                for key in p:\n",
    "                    best_out += key + '=' + str(p[key])+', '\n",
    "                    best_params[key] = p[key]\n",
    "                best_out = best_out[:-2]\n",
    "\n",
    "            #show results of cv\n",
    "            if verbose:\n",
    "                print(\"Parameters: \"+out)\n",
    "                print(\"Mean train \"+eval_method+\": \"+str(mean_eval_train) + \" Mean test \"+eval_method+\": \"+str(mean_eval_test) + \" Iteration: \"+str(iteration))\n",
    "                print(str(done)+\"/\"+str(amount))\n",
    "                \n",
    "                if (time.time()-timer) > 300:\n",
    "                    print(str((time.time()-start)/60) +\"m elapsed. Finished \"+str(100*done/amount)+\"%\" )\n",
    "                    timer = time.time()\n",
    "                print(\"\")\n",
    "            pbar.update(1)\n",
    "    stop = time.time()\n",
    "    if verbose:\n",
    "        print(\"----------------------------------------\")\n",
    "    print(f\"Tuning time: {(time.time()-start)}s\")\n",
    "    print(\"Best parameters: \"+best_out+\":\")\n",
    "    print(\"Best mean train \"+eval_method+\": \"+str(best_eval_train) + \" Best mean test \"+eval_method+\": \"+str(best_eval) + \" Best iteration: \"+str(iteration))\n",
    "    \n",
    "    #show accuracy score\n",
    "    bst = xgb.train(dict({'random_state':randomSeed,'eval_metric':eval_method,'num_class':num_class},**best_params), train, best_iter+1)\n",
    "    test_preds = bst.predict(test)\n",
    "    train_preds = bst.predict(train)\n",
    "\n",
    "    print(\"Test accuracy:\",accuracy_score(test_labels,test_preds),\"Train accuracy:\",accuracy_score(train_labels,train_preds))\n",
    "    return best_params,best_iter+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'tree_method':'hist',\n",
    "    'n_jobs':-1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = [\n",
    "    dict({'learning_rate':lr},**params) \n",
    "    for lr in [0.001,0.005,0.01,0.05,0.1,0.2,0.25,0.3,0.35,0.4,0.5,0.7]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params,_ = param_search(grid,dtrain,dtest,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = [\n",
    "    dict({'max_depth':i,'min_child_weight':j},**params) \n",
    "    for i in [1,3,6,9,12]\n",
    "    for j in [1,3,5,8,10]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params,_ = param_search(grid,dtrain,dtest,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = [\n",
    "    dict({'gamma':g},**params) \n",
    "    for g in [0,1,2,3,4,5]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params,_ = param_search(grid,dtrain,dtest,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = [\n",
    "    dict({'subsample':i,'colsample_bytree':j},**params) \n",
    "    for i in [0.7,0.8,0.9,1]\n",
    "    for j in [0.7,0.8,0.9,1]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params,_ = param_search(grid,dtrain,dtest,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = [\n",
    "    dict({'reg_lambda':i},**params) \n",
    "    for i in [1,5,10,15,20,25]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params,estimators = param_search(grid,dtrain,dtest,y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(n_estimators = estimators, random_state=rnd,**params,eval_metric=\"mae\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_new,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = model.predict(X_test_new)\n",
    "pred_train = model.predict(X_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_results(model,X,y,title=\"\"):\n",
    "   \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.suptitle(title)\n",
    "    cm = plot_confusion_matrix(model,X,y,display_labels=[\"Red\",\"Blue\"],values_format=\"d\",ax=ax1)\n",
    "    fig.set_size_inches(20,10)\n",
    "    num = cm.text_[0][0].get_text()\n",
    "    txt = \"True Negative\"+ \"\\n\" + num + \"\\n\" + '{0:.3f}'.format(100*(int(num)/len(y)))+\"%\"\n",
    "    cm.text_[0][0].set_text(txt)\n",
    "    \n",
    "    num = cm.text_[1][0].get_text()\n",
    "    txt = \"False Negative\"+ \"\\n\" + num + \"\\n\" + '{0:.3f}'.format(100*(int(num)/len(y)))+\"%\"\n",
    "    cm.text_[1][0].set_text(txt)\n",
    "\n",
    "    num = cm.text_[0][1].get_text()\n",
    "    txt = \"False Positive\"+ \"\\n\" + num + \"\\n\" + '{0:.3f}'.format(100*(int(num)/len(y)))+\"%\"\n",
    "    cm.text_[0][1].set_text(txt)\n",
    "\n",
    "    num = cm.text_[1][1].get_text()\n",
    "    txt = \"True Positive\"+ \"\\n\" + num + \"\\n\" + '{0:.3f}'.format(100*(int(num)/len(y)))+\"%\"\n",
    "    cm.text_[1][1].set_text(txt)\n",
    "    \n",
    "    falsePosRate, truePosRate, _ = roc_curve(y,np.random.choice([0,1],size=len(y)))\n",
    "    plot_roc_curve(model,X,y,ax=ax2)\n",
    "    ax2.plot(falsePosRate, truePosRate, linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train accuracy:\",accuracy_score(y_train,pred_train))\n",
    "print(classification_report(y_train,pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_model_results(model,X_train_new,y_train,title=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_accuracy = accuracy_score(y_test,pred_test)\n",
    "print(\"Test accuracy:\",model_accuracy)\n",
    "print(classification_report(y_test,pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_model_results(model,X_test_new,y_test,title=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(ncols=4,figsize=(28,8),sharex=True)\n",
    "cms = [None,None,None,None]\n",
    "cms[0] = plot_confusion_matrix(knn5_clf,testD,testE,ax=axes[0],display_labels=[\"Red\",\"Blue\"],values_format=\"d\")\n",
    "cms[1] = plot_confusion_matrix(knn_clf,testD,testE,ax=axes[1],display_labels=[\"Red\",\"Blue\"],values_format=\"d\")\n",
    "cms[2] = plot_confusion_matrix(svc_clf,testD,testE,ax=axes[2],display_labels=[\"Red\",\"Blue\"],values_format=\"d\")\n",
    "cms[3] = plot_confusion_matrix(model,X_test_new,y_test,ax=axes[3],display_labels=[\"Red\",\"Blue\"],values_format=\"d\")\n",
    "vals = [[\"True Neg\",\"False Pos\"],[\"False Neg\",\"True Pos\"]]\n",
    "for cm in cms:\n",
    "    text = [[cm.text_[i][j].get_text() for j in range(len(cm.text_[i]))] for i in range(len(cm.text_))]\n",
    "    for i in range(len(text)):\n",
    "        for t in range(len(text[i])):\n",
    "            txt = vals[i][t] + \"\\n\" + text[i][t] + \"\\n\" + '{0:.3f}'.format(100*(int(text[i][t])/len(testE)))+\"%\"\n",
    "            cm.text_[i][t].set_text(txt)\n",
    "        \n",
    "axes[0].title.set_text(\"KNN\\nn = 5\\n\"+str(knn5_accuracy))\n",
    "axes[1].title.set_text(\"KNN\\nn =\"+str(knn_clf.n_neighbors)+\"\\n\"+str(knn_accuracy))\n",
    "axes[2].title.set_text(\"SVC\\nC =\"+str('{0:.2f}'.format(svc_clf.C))+\"\\n\"+str(svc_accuracy))\n",
    "axes[3].title.set_text(\"XGBoost\\n\"+str(model_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,12))\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "falsePosRate, truePosRate, _ = roc_curve(testE,np.random.choice([0,1],size=len(testE)))\n",
    "plot_roc_curve(knn5_clf,testD,testE,ax=ax,name=\"KNN, n = 5\")\n",
    "plot_roc_curve(knn_clf,testD,testE,ax=ax,name=\"KNN, n =\"+str(knn_clf.n_neighbors))\n",
    "plot_roc_curve(svc_clf,testD,testE,ax=ax,name=\"SVC, C =\"+str('{0:.2f}'.format(svc_clf.C)))\n",
    "plot_roc_curve(model,X_test_new,y_test,ax=ax,name=\"XGBoost\")\n",
    "\n",
    "ax.plot(falsePosRate, truePosRate, linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u><b> Conclusions </b></u>\n",
    "\n",
    "Not only have we improved our model's accuracy, we also managed to do so using only 1 feature instead of 8 like with the old model, however, like explained prior we only reached an accuracy of 72.5%.\n",
    "\n",
    "Although we managed to improve only by about 1% we managed to do so using 1/8 the amount of features so we could say this is a major improvement from the last model."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1baa965d5efe3ac65b79dfc60c0d706280b1da80fedb7760faf2759126c4f253"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
